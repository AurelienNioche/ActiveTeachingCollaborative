{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Train a Mario-playing RL Agent\n",
    "================\n",
    "\n",
    "Authors: `Yuansong Feng <https://github.com/YuansongFeng>`__, `Suraj\n",
    "Subramanian <https://github.com/suraj813>`__, `Howard\n",
    "Wang <https://github.com/hw26>`__, `Steven\n",
    "Guo <https://github.com/GuoYuzhang>`__.\n",
    "\n",
    "\n",
    "This tutorial walks you through the fundamentals of Deep Reinforcement\n",
    "Learning. At the end, you will implement an AI-powered Mario (using\n",
    "`Double Deep Q-Networks <https://arxiv.org/pdf/1509.06461.pdf>`__) that\n",
    "can play the game by itself.\n",
    "\n",
    "Although no prior knowledge of RL is necessary for this tutorial, you\n",
    "can familiarize yourself with these RL\n",
    "`concepts <https://spinningup.openai.com/en/latest/spinningup/rl_intro.html>`__,\n",
    "and have this handy\n",
    "`cheatsheet <https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N>`__\n",
    "as your companion. The full code is available\n",
    "`here <https://github.com/yuansongFeng/MadMario/>`__.\n",
    "\n",
    ".. figure:: /_static/img/mario.gif\n",
    "   :alt: mario\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym-super-mario-bros==7.3.0\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os, copy\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL Definitions\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "**Environment** The world that an agent interacts with and learns from.\n",
    "\n",
    "**Action** $a$ : How the Agent responds to the Environment. The\n",
    "set of all possible Actions is called *action-space*.\n",
    "\n",
    "**State** $s$ : The current characteristic of the Environment. The\n",
    "set of all possible States the Environment can be in is called\n",
    "*state-space*.\n",
    "\n",
    "**Reward** $r$ : Reward is the key feedback from Environment to\n",
    "Agent. It is what drives the Agent to learn and to change its future\n",
    "action. An aggregation of rewards over multiple time steps is called\n",
    "**Return**.\n",
    "\n",
    "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected\n",
    "return if you start in state $s$, take an arbitrary action\n",
    "$a$, and then for each future time step take the action that\n",
    "maximizes returns. $Q$ can be said to stand for the “quality” of\n",
    "the action in a state. We try to approximate this function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "Initialize Environment\n",
    "------------------------\n",
    "\n",
    "In Mario, the environment consists of tubes, mushrooms and other\n",
    "components.\n",
    "\n",
    "When Mario makes an action, the environment responds with the changed\n",
    "(next) state, reward and other info.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\")\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Environment\n",
    "------------------------\n",
    "\n",
    "Environment data is returned to the agent in ``next_state``. As you saw\n",
    "above, each state is represented by a ``[3, 240, 256]`` size array.\n",
    "Often that is more information than our agent needs; for instance,\n",
    "Mario’s actions do not depend on the color of the pipes or the sky!\n",
    "\n",
    "We use **Wrappers** to preprocess environment data before sending it to\n",
    "the agent.\n",
    "\n",
    "``GrayScaleObservation`` is a common wrapper to transform an RGB image\n",
    "to grayscale; doing so reduces the size of the state representation\n",
    "without losing useful information. Now the size of each state:\n",
    "``[1, 240, 256]``\n",
    "\n",
    "``ResizeObservation`` downsamples each observation into a square image.\n",
    "New size: ``[1, 84, 84]``\n",
    "\n",
    "``SkipFrame`` is a custom wrapper that inherits from ``gym.Wrapper`` and\n",
    "implements the ``step()`` function. Because consecutive frames don’t\n",
    "vary much, we can skip n-intermediate frames without losing much\n",
    "information. The n-th frame aggregates rewards accumulated over each\n",
    "skipped frame.\n",
    "\n",
    "``FrameStack`` is a wrapper that allows us to squash consecutive frames\n",
    "of the environment into a single observation point to feed to our\n",
    "learning model. This way, we can identify if Mario was landing or\n",
    "jumping based on the direction of his movement in the previous several\n",
    "frames.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # permute [H, W, C] array to [C, H, W] tensor\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        transform = T.Grayscale()\n",
    "        observation = transform(observation)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        transforms = T.Compose(\n",
    "            [T.Resize(self.shape), T.Normalize(0, 255)]\n",
    "        )\n",
    "        observation = transforms(observation).squeeze(0)\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After applying the above wrappers to the environment, the final wrapped\n",
    "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
    "shown above in the image on the left. Each time Mario makes an action,\n",
    "the environment responds with a state of this structure. The structure\n",
    "is represented by a 3-D array of size ``[4, 84, 84]``.\n",
    "\n",
    ".. figure:: /_static/img/mario_env.png\n",
    "   :alt: picture\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent\n",
    "\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "We create a class ``Mario`` to represent our agent in the game. Mario\n",
    "should be able to:\n",
    "\n",
    "-  **Act** according to the optimal action policy based on the current\n",
    "   state (of the environment).\n",
    "\n",
    "-  **Remember** experiences. Experience = (current state, current\n",
    "   action, reward, next state). Mario *caches* and later *recalls* his\n",
    "   experiences to update his action policy.\n",
    "\n",
    "-  **Learn** a better action policy over time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections, we will populate Mario’s parameters and\n",
    "define his functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Act\n",
    "--------------\n",
    "\n",
    "For any given state, an agent can choose to do the most optimal action\n",
    "(**exploit**) or a random action (**explore**).\n",
    "\n",
    "Mario randomly explores with a chance of ``self.exploration_rate``; when\n",
    "he chooses to exploit, he relies on ``MarioNet`` (implemented in\n",
    "``Learn`` section) to provide the most optimal action.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.use_cuda = torch.cuda.is_available()\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        if self.use_cuda:\n",
    "            self.net = self.net.to(device=\"cuda\")\n",
    "\n",
    "        self.exploration_rate = 1\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    action_idx (int): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = state.__array__()\n",
    "            if self.use_cuda:\n",
    "                state = torch.tensor(state).cuda()\n",
    "            else:\n",
    "                state = torch.tensor(state)\n",
    "            state = state.unsqueeze(0)\n",
    "            action_values = self.net(state, model=\"online\")\n",
    "            action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache and Recall\n",
    "----------------------\n",
    "\n",
    "These two functions serve as Mario’s “memory” process.\n",
    "\n",
    "``cache()``: Each time Mario performs an action, he stores the\n",
    "``experience`` to his memory. His experience includes the current\n",
    "*state*, *action* performed, *reward* from the action, the *next state*,\n",
    "and whether the game is *done*.\n",
    "\n",
    "``recall()``: Mario randomly samples a batch of experiences from his\n",
    "memory, and uses that to learn the game.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):  # subclassing for continuity\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state (LazyFrame),\n",
    "        next_state (LazyFrame),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done(bool))\n",
    "        \"\"\"\n",
    "        state = state.__array__()\n",
    "        next_state = next_state.__array__()\n",
    "\n",
    "        if self.use_cuda:\n",
    "            state = torch.tensor(state).cuda()\n",
    "            next_state = torch.tensor(next_state).cuda()\n",
    "            action = torch.tensor([action]).cuda()\n",
    "            reward = torch.tensor([reward]).cuda()\n",
    "            done = torch.tensor([done]).cuda()\n",
    "        else:\n",
    "            state = torch.tensor(state)\n",
    "            next_state = torch.tensor(next_state)\n",
    "            action = torch.tensor([action])\n",
    "            reward = torch.tensor([reward])\n",
    "            done = torch.tensor([done])\n",
    "\n",
    "        self.memory.append((state, next_state, action, reward, done,))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn\n",
    "--------------\n",
    "\n",
    "Mario uses the `DDQN algorithm <https://arxiv.org/pdf/1509.06461>`__\n",
    "under the hood. DDQN uses two ConvNets - $Q_{online}$ and\n",
    "$Q_{target}$ - that independently approximate the optimal\n",
    "action-value function.\n",
    "\n",
    "In our implementation, we share feature generator ``features`` across\n",
    "$Q_{online}$ and $Q_{target}$, but maintain separate FC\n",
    "classifiers for each. $\\theta_{target}$ (the parameters of\n",
    "$Q_{target}$) is frozen to prevent updation by backprop. Instead,\n",
    "it is periodically synced with $\\theta_{online}$ (more on this\n",
    "later).\n",
    "\n",
    "Neural Network\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "    \"\"\"mini cnn structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "        )\n",
    "\n",
    "        self.target = copy.deepcopy(self.online)\n",
    "\n",
    "        # Q_target parameters are frozen.\n",
    "        for p in self.target.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, input, model):\n",
    "        if model == \"online\":\n",
    "            return self.online(input)\n",
    "        elif model == \"target\":\n",
    "            return self.target(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TD Estimate & TD Target\n",
    "\n",
    "\n",
    "Two values are involved in learning:\n",
    "\n",
    "**TD Estimate** - the predicted optimal $Q^*$ for a given state\n",
    "$s$\n",
    "\n",
    "\\begin{align}{TD}_e = Q_{online}^*(s,a)\\end{align}\n",
    "\n",
    "**TD Target** - aggregation of current reward and the estimated\n",
    "$Q^*$ in the next state $s'$\n",
    "\n",
    "\\begin{align}a' = argmax_{a} Q_{online}(s', a)\\end{align}\n",
    "\n",
    "\\begin{align}{TD}_t = r + \\gamma Q_{target}^*(s',a')\\end{align}\n",
    "\n",
    "Because we don’t know what next action $a'$ will be, we use the\n",
    "action $a'$ maximizes $Q_{online}$ in the next state\n",
    "$s'$.\n",
    "\n",
    "Notice we use the\n",
    "`@torch.no_grad() <https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad>`__\n",
    "decorator on ``td_target()`` to disable gradient calculations here\n",
    "(because we don’t need to backpropagate on $\\theta_{target}$).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, model=\"online\")[\n",
    "            np.arange(0, self.batch_size), action\n",
    "        ]  # Q_online(s,a)\n",
    "        return current_Q\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q = self.net(next_state, model=\"online\")\n",
    "        best_action = torch.argmax(next_state_Q, axis=1)\n",
    "        next_Q = self.net(next_state, model=\"target\")[\n",
    "            np.arange(0, self.batch_size), best_action\n",
    "        ]\n",
    "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the model\n",
    "\n",
    "As Mario samples inputs from his replay buffer, we compute $TD_t$\n",
    "and $TD_e$ and backpropagate this loss down $Q_{online}$ to\n",
    "update its parameters $\\theta_{online}$ ($\\alpha$ is the\n",
    "learning rate ``lr`` passed to the ``optimizer``)\n",
    "\n",
    "\\begin{align}\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\end{align}\n",
    "\n",
    "$\\theta_{target}$ does not update through backpropagation.\n",
    "Instead, we periodically copy $\\theta_{online}$ to\n",
    "$\\theta_{target}$\n",
    "\n",
    "\\begin{align}\\theta_{target} \\leftarrow \\theta_{online}\\end{align}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target):\n",
    "        loss = self.loss_fn(td_estimate, td_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.load_state_dict(self.net.online.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save checkpoint\n",
    "~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = (\n",
    "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        )\n",
    "        torch.save(\n",
    "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
    "            save_path,\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        # Sample from memory\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        # Get TD Estimate\n",
    "        td_est = self.td_estimate(state, action)\n",
    "\n",
    "        # Get TD Target\n",
    "        td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "        # Backpropagate loss through Q_online\n",
    "        loss = self.update_Q_online(td_est, td_tgt)\n",
    "\n",
    "        return (td_est.mean().item(), loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging\n",
    "--------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s play!\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "In this example we run the training loop for 10 episodes, but for Mario to truly learn the ways of\n",
    "his world, we suggest running the loop for at least 40,000 episodes!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-3923307638b1>:32: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.tensor([done])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 40 - Epsilon 0.9999900000487484 - Mean Reward 231.0 - Mean Length 40.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.489 - Time 2021-04-30T17:59:49\n",
      "Episode 20 - Step 3071 - Epsilon 0.9992325445486191 - Mean Reward 571.095 - Mean Length 146.238 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 27.714 - Time 2021-04-30T18:00:17\n",
      "Episode 40 - Step 6784 - Epsilon 0.9983054371833989 - Mean Reward 557.463 - Mean Length 165.463 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 35.111 - Time 2021-04-30T18:00:52\n",
      "Episode 60 - Step 10180 - Epsilon 0.9974582354492343 - Mean Reward 567.557 - Mean Length 166.885 - Mean Loss 0.048 - Mean Q Value 0.06 - Time Delta 33.384 - Time 2021-04-30T18:01:25\n",
      "Episode 80 - Step 15199 - Epsilon 0.9962074594414826 - Mean Reward 585.383 - Mean Length 187.642 - Mean Loss 0.265 - Mean Q Value 0.904 - Time Delta 157.767 - Time 2021-04-30T18:04:03\n",
      "Episode 100 - Step 19492 - Epsilon 0.9951388531953127 - Mean Reward 599.74 - Mean Length 194.52 - Mean Loss 0.305 - Mean Q Value 1.369 - Time Delta 137.691 - Time 2021-04-30T18:06:21\n",
      "Episode 120 - Step 23695 - Epsilon 0.994093760076486 - Mean Reward 620.8 - Mean Length 206.24 - Mean Loss 0.414 - Mean Q Value 2.527 - Time Delta 138.584 - Time 2021-04-30T18:08:39\n",
      "Episode 140 - Step 26123 - Epsilon 0.9934905281880316 - Mean Reward 602.41 - Mean Length 193.39 - Mean Loss 0.517 - Mean Q Value 3.757 - Time Delta 84.063 - Time 2021-04-30T18:10:03\n",
      "Episode 160 - Step 30620 - Epsilon 0.9923742239421264 - Mean Reward 604.13 - Mean Length 204.4 - Mean Loss 0.581 - Mean Q Value 5.034 - Time Delta 156.298 - Time 2021-04-30T18:12:40\n",
      "Episode 180 - Step 36861 - Epsilon 0.9908270791458004 - Mean Reward 631.49 - Mean Length 216.62 - Mean Loss 0.503 - Mean Q Value 5.981 - Time Delta 216.354 - Time 2021-04-30T18:16:16\n",
      "Episode 200 - Step 39827 - Epsilon 0.9900926530968164 - Mean Reward 625.69 - Mean Length 203.35 - Mean Loss 0.505 - Mean Q Value 6.997 - Time Delta 102.398 - Time 2021-04-30T18:17:58\n",
      "Episode 220 - Step 44173 - Epsilon 0.9890175014765625 - Mean Reward 635.49 - Mean Length 204.78 - Mean Loss 0.525 - Mean Q Value 7.903 - Time Delta 152.233 - Time 2021-04-30T18:20:31\n",
      "Episode 240 - Step 47817 - Epsilon 0.9881169166986705 - Mean Reward 656.8 - Mean Length 216.94 - Mean Loss 0.532 - Mean Q Value 8.825 - Time Delta 128.942 - Time 2021-04-30T18:22:40\n",
      "Episode 260 - Step 54248 - Epsilon 0.9865295479152143 - Mean Reward 701.22 - Mean Length 236.28 - Mean Loss 0.559 - Mean Q Value 9.843 - Time Delta 227.324 - Time 2021-04-30T18:26:27\n",
      "Episode 280 - Step 58129 - Epsilon 0.9855728317036304 - Mean Reward 669.24 - Mean Length 212.68 - Mean Loss 0.579 - Mean Q Value 10.658 - Time Delta 140.505 - Time 2021-04-30T18:28:47\n",
      "Episode 300 - Step 61303 - Epsilon 0.9847910897609815 - Mean Reward 676.34 - Mean Length 214.76 - Mean Loss 0.618 - Mean Q Value 11.608 - Time Delta 123.397 - Time 2021-04-30T18:30:51\n",
      "Episode 320 - Step 64757 - Epsilon 0.9839410895889964 - Mean Reward 653.08 - Mean Length 205.84 - Mean Loss 0.629 - Mean Q Value 12.447 - Time Delta 151.882 - Time 2021-04-30T18:33:23\n",
      "Episode 340 - Step 69022 - Epsilon 0.9828925213882698 - Mean Reward 691.5 - Mean Length 212.05 - Mean Loss 0.667 - Mean Q Value 13.228 - Time Delta 202.303 - Time 2021-04-30T18:36:45\n",
      "Episode 360 - Step 74110 - Epsilon 0.9816430767599593 - Mean Reward 687.24 - Mean Length 198.62 - Mean Loss 0.701 - Mean Q Value 14.115 - Time Delta 274.817 - Time 2021-04-30T18:41:20\n",
      "Episode 380 - Step 77164 - Epsilon 0.9808938782200347 - Mean Reward 682.79 - Mean Length 190.35 - Mean Loss 0.727 - Mean Q Value 14.981 - Time Delta 180.902 - Time 2021-04-30T18:44:21\n",
      "Episode 400 - Step 82659 - Epsilon 0.9795473002288478 - Mean Reward 690.27 - Mean Length 213.56 - Mean Loss 0.76 - Mean Q Value 15.743 - Time Delta 382.488 - Time 2021-04-30T18:50:43\n",
      "Episode 420 - Step 86614 - Epsilon 0.9785792513726261 - Mean Reward 695.7 - Mean Length 218.57 - Mean Loss 0.774 - Mean Q Value 16.369 - Time Delta 291.848 - Time 2021-04-30T18:55:35\n",
      "Episode 440 - Step 91345 - Epsilon 0.9774225208146916 - Mean Reward 671.16 - Mean Length 223.23 - Mean Loss 0.772 - Mean Q Value 17.021 - Time Delta 360.893 - Time 2021-04-30T19:01:36\n",
      "Episode 460 - Step 95191 - Epsilon 0.9764831806035554 - Mean Reward 633.46 - Mean Length 210.81 - Mean Loss 0.779 - Mean Q Value 17.575 - Time Delta 311.857 - Time 2021-04-30T19:06:48\n",
      "Episode 480 - Step 100594 - Epsilon 0.975165086190832 - Mean Reward 656.92 - Mean Length 234.3 - Mean Loss 0.775 - Mean Q Value 18.028 - Time Delta 460.971 - Time 2021-04-30T19:14:29\n",
      "Episode 500 - Step 104656 - Epsilon 0.9741753085665916 - Mean Reward 647.65 - Mean Length 219.97 - Mean Loss 0.766 - Mean Q Value 18.697 - Time Delta 372.832 - Time 2021-04-30T19:20:42\n",
      "Episode 520 - Step 107549 - Epsilon 0.9734709909161056 - Mean Reward 622.21 - Mean Length 209.35 - Mean Loss 0.765 - Mean Q Value 19.201 - Time Delta 275.1 - Time 2021-04-30T19:25:17\n",
      "Episode 540 - Step 113676 - Epsilon 0.9719810179636721 - Mean Reward 628.66 - Mean Length 223.31 - Mean Loss 0.788 - Mean Q Value 19.695 - Time Delta 564.436 - Time 2021-04-30T19:34:41\n",
      "Episode 560 - Step 118108 - Epsilon 0.970904659273906 - Mean Reward 637.47 - Mean Length 229.17 - Mean Loss 0.792 - Mean Q Value 20.048 - Time Delta 403.277 - Time 2021-04-30T19:41:24\n",
      "Episode 580 - Step 120672 - Epsilon 0.9702825087300083 - Mean Reward 602.41 - Mean Length 200.78 - Mean Loss 0.805 - Mean Q Value 20.441 - Time Delta 266.528 - Time 2021-04-30T19:45:51\n",
      "Episode 600 - Step 125373 - Epsilon 0.9691428538898542 - Mean Reward 615.43 - Mean Length 207.17 - Mean Loss 0.828 - Mean Q Value 20.733 - Time Delta 445.015 - Time 2021-04-30T19:53:16\n",
      "Episode 620 - Step 128358 - Epsilon 0.9684199007301174 - Mean Reward 627.05 - Mean Length 208.09 - Mean Loss 0.833 - Mean Q Value 20.963 - Time Delta 289.139 - Time 2021-04-30T19:58:05\n",
      "Episode 640 - Step 134127 - Epsilon 0.9670242036676062 - Mean Reward 635.14 - Mean Length 204.51 - Mean Loss 0.837 - Mean Q Value 21.35 - Time Delta 554.085 - Time 2021-04-30T20:07:19\n",
      "Episode 660 - Step 137900 - Epsilon 0.966112488028888 - Mean Reward 624.6 - Mean Length 197.92 - Mean Loss 0.833 - Mean Q Value 21.715 - Time Delta 373.569 - Time 2021-04-30T20:13:33\n",
      "Episode 680 - Step 142589 - Epsilon 0.964980626065362 - Mean Reward 658.81 - Mean Length 219.17 - Mean Loss 0.836 - Mean Q Value 22.032 - Time Delta 442.544 - Time 2021-04-30T20:20:55\n",
      "Episode 700 - Step 148529 - Epsilon 0.9635486931298128 - Mean Reward 666.43 - Mean Length 231.56 - Mean Loss 0.822 - Mean Q Value 22.145 - Time Delta 586.238 - Time 2021-04-30T20:30:42\n",
      "Episode 720 - Step 152675 - Epsilon 0.9625504921914712 - Mean Reward 684.14 - Mean Length 243.17 - Mean Loss 0.841 - Mean Q Value 22.482 - Time Delta 405.075 - Time 2021-04-30T20:37:27\n",
      "Episode 740 - Step 155729 - Epsilon 0.9618158652782621 - Mean Reward 660.81 - Mean Length 216.02 - Mean Loss 0.853 - Mean Q Value 22.678 - Time Delta 299.959 - Time 2021-04-30T20:42:27\n",
      "Episode 760 - Step 160142 - Epsilon 0.9607553269192229 - Mean Reward 667.56 - Mean Length 222.42 - Mean Loss 0.858 - Mean Q Value 22.853 - Time Delta 445.372 - Time 2021-04-30T20:49:52\n",
      "Episode 780 - Step 163420 - Epsilon 0.9599683103544081 - Mean Reward 646.01 - Mean Length 208.31 - Mean Loss 0.883 - Mean Q Value 23.096 - Time Delta 327.137 - Time 2021-04-30T20:55:19\n",
      "Episode 800 - Step 166768 - Epsilon 0.9591651529463943 - Mean Reward 626.23 - Mean Length 182.39 - Mean Loss 0.898 - Mean Q Value 23.371 - Time Delta 310.094 - Time 2021-04-30T21:00:29\n",
      "Episode 820 - Step 170469 - Epsilon 0.9582780957157573 - Mean Reward 620.2 - Mean Length 177.94 - Mean Loss 0.912 - Mean Q Value 23.491 - Time Delta 366.221 - Time 2021-04-30T21:06:35\n",
      "Episode 840 - Step 174241 - Epsilon 0.957374865298543 - Mean Reward 630.88 - Mean Length 185.12 - Mean Loss 0.905 - Mean Q Value 23.685 - Time Delta 366.701 - Time 2021-04-30T21:12:42\n",
      "Episode 860 - Step 177932 - Epsilon 0.9564918549926246 - Mean Reward 625.17 - Mean Length 177.9 - Mean Loss 0.912 - Mean Q Value 23.864 - Time Delta 352.785 - Time 2021-04-30T21:18:35\n",
      "Episode 880 - Step 181713 - Epsilon 0.9555881581306214 - Mean Reward 633.59 - Mean Length 182.93 - Mean Loss 0.934 - Mean Q Value 23.978 - Time Delta 367.851 - Time 2021-04-30T21:24:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900 - Step 187003 - Epsilon 0.954325227930119 - Mean Reward 655.73 - Mean Length 202.35 - Mean Loss 0.951 - Mean Q Value 24.27 - Time Delta 502.065 - Time 2021-04-30T21:33:05\n",
      "Episode 920 - Step 190627 - Epsilon 0.9534610007195089 - Mean Reward 648.61 - Mean Length 201.58 - Mean Loss 0.945 - Mean Q Value 24.557 - Time Delta 349.42 - Time 2021-04-30T21:38:54\n",
      "Episode 940 - Step 193939 - Epsilon 0.9526718616608149 - Mean Reward 636.59 - Mean Length 196.98 - Mean Loss 0.968 - Mean Q Value 24.745 - Time Delta 320.404 - Time 2021-04-30T21:44:15\n",
      "Episode 960 - Step 198776 - Epsilon 0.9515205393265913 - Mean Reward 685.06 - Mean Length 208.44 - Mean Loss 0.977 - Mean Q Value 25.072 - Time Delta 458.483 - Time 2021-04-30T21:51:53\n",
      "Episode 980 - Step 202921 - Epsilon 0.9505350367460994 - Mean Reward 698.49 - Mean Length 212.08 - Mean Loss 0.978 - Mean Q Value 25.448 - Time Delta 402.103 - Time 2021-04-30T21:58:35\n",
      "Episode 1000 - Step 206735 - Epsilon 0.9496291334332339 - Mean Reward 681.53 - Mean Length 197.32 - Mean Loss 1.001 - Mean Q Value 25.703 - Time Delta 397.202 - Time 2021-04-30T22:05:12\n",
      "Episode 1020 - Step 211565 - Epsilon 0.9484831481390662 - Mean Reward 704.84 - Mean Length 209.38 - Mean Loss 1.04 - Mean Q Value 26.069 - Time Delta 487.185 - Time 2021-04-30T22:13:20\n",
      "Episode 1040 - Step 217275 - Epsilon 0.9471301542049598 - Mean Reward 721.82 - Mean Length 233.36 - Mean Loss 1.052 - Mean Q Value 26.401 - Time Delta 554.945 - Time 2021-04-30T22:22:35\n",
      "Episode 1060 - Step 222242 - Epsilon 0.9459547850972161 - Mean Reward 701.66 - Mean Length 234.66 - Mean Loss 1.081 - Mean Q Value 26.581 - Time Delta 507.373 - Time 2021-04-30T22:31:02\n",
      "Episode 1080 - Step 225905 - Epsilon 0.945088923411338 - Mean Reward 683.49 - Mean Length 229.84 - Mean Loss 1.08 - Mean Q Value 26.732 - Time Delta 357.885 - Time 2021-04-30T22:37:00\n",
      "Episode 1100 - Step 231078 - Epsilon 0.9438674769963077 - Mean Reward 718.29 - Mean Length 243.43 - Mean Loss 1.074 - Mean Q Value 26.871 - Time Delta 517.794 - Time 2021-04-30T22:45:38\n",
      "Episode 1120 - Step 235193 - Epsilon 0.9428969724967461 - Mean Reward 704.22 - Mean Length 236.28 - Mean Loss 1.058 - Mean Q Value 27.018 - Time Delta 413.847 - Time 2021-04-30T22:52:31\n",
      "Episode 1140 - Step 238666 - Epsilon 0.9420786574004234 - Mean Reward 696.76 - Mean Length 213.91 - Mean Loss 1.045 - Mean Q Value 27.196 - Time Delta 334.168 - Time 2021-04-30T22:58:06\n",
      "Episode 1160 - Step 243082 - Epsilon 0.9410391763317034 - Mean Reward 677.24 - Mean Length 208.4 - Mean Loss 1.055 - Mean Q Value 27.503 - Time Delta 466.52 - Time 2021-04-30T23:05:52\n",
      "Episode 1180 - Step 247341 - Epsilon 0.940037737978739 - Mean Reward 698.8 - Mean Length 214.36 - Mean Loss 1.056 - Mean Q Value 27.805 - Time Delta 420.436 - Time 2021-04-30T23:12:53\n",
      "Episode 1200 - Step 253380 - Epsilon 0.9386195866227333 - Mean Reward 700.82 - Mean Length 223.02 - Mean Loss 1.05 - Mean Q Value 28.12 - Time Delta 595.14 - Time 2021-04-30T23:22:48\n",
      "Episode 1220 - Step 257288 - Epsilon 0.937703002995916 - Mean Reward 688.59 - Mean Length 220.95 - Mean Loss 1.071 - Mean Q Value 28.273 - Time Delta 378.141 - Time 2021-04-30T23:29:06\n",
      "Episode 1240 - Step 261070 - Epsilon 0.936816823702966 - Mean Reward 687.85 - Mean Length 224.04 - Mean Loss 1.085 - Mean Q Value 28.398 - Time Delta 363.484 - Time 2021-04-30T23:35:09\n",
      "Episode 1260 - Step 264168 - Epsilon 0.9360915398846575 - Mean Reward 681.24 - Mean Length 210.86 - Mean Loss 1.085 - Mean Q Value 28.467 - Time Delta 294.366 - Time 2021-04-30T23:40:04\n",
      "Episode 1280 - Step 267307 - Epsilon 0.9353572301193197 - Mean Reward 651.84 - Mean Length 199.66 - Mean Loss 1.09 - Mean Q Value 28.533 - Time Delta 308.793 - Time 2021-04-30T23:45:12\n",
      "Episode 1300 - Step 271109 - Epsilon 0.9344685953514654 - Mean Reward 609.25 - Mean Length 177.29 - Mean Loss 1.104 - Mean Q Value 28.577 - Time Delta 373.017 - Time 2021-04-30T23:51:26\n",
      "Episode 1320 - Step 274617 - Epsilon 0.9336494255492936 - Mean Reward 613.56 - Mean Length 173.29 - Mean Loss 1.105 - Mean Q Value 28.63 - Time Delta 348.428 - Time 2021-04-30T23:57:14\n",
      "Episode 1340 - Step 278794 - Epsilon 0.9326749708904116 - Mean Reward 620.15 - Mean Length 177.24 - Mean Loss 1.108 - Mean Q Value 28.698 - Time Delta 416.321 - Time 2021-05-01T00:04:10\n",
      "Episode 1360 - Step 282894 - Epsilon 0.9317194687042025 - Mean Reward 620.43 - Mean Length 187.26 - Mean Loss 1.108 - Mean Q Value 28.721 - Time Delta 397.504 - Time 2021-05-01T00:10:48\n",
      "Episode 1380 - Step 286804 - Epsilon 0.9308091577965696 - Mean Reward 626.75 - Mean Length 194.97 - Mean Loss 1.106 - Mean Q Value 28.798 - Time Delta 412.141 - Time 2021-05-01T00:17:40\n",
      "Episode 1400 - Step 291136 - Epsilon 0.9298016370235693 - Mean Reward 633.42 - Mean Length 200.27 - Mean Loss 1.1 - Mean Q Value 28.745 - Time Delta 425.46 - Time 2021-05-01T00:24:45\n",
      "Episode 1420 - Step 295316 - Epsilon 0.9288305016979307 - Mean Reward 625.23 - Mean Length 206.99 - Mean Loss 1.092 - Mean Q Value 28.84 - Time Delta 402.769 - Time 2021-05-01T00:31:28\n",
      "Episode 1440 - Step 299561 - Epsilon 0.9278453030682042 - Mean Reward 634.74 - Mean Length 207.67 - Mean Loss 1.083 - Mean Q Value 28.821 - Time Delta 407.829 - Time 2021-05-01T00:38:16\n",
      "Episode 1460 - Step 303949 - Epsilon 0.9268280147281063 - Mean Reward 644.16 - Mean Length 210.55 - Mean Loss 1.073 - Mean Q Value 28.863 - Time Delta 421.948 - Time 2021-05-01T00:45:18\n",
      "Episode 1480 - Step 307694 - Epsilon 0.9259606779761531 - Mean Reward 654.22 - Mean Length 208.9 - Mean Loss 1.068 - Mean Q Value 28.865 - Time Delta 357.215 - Time 2021-05-01T00:51:15\n",
      "Episode 1500 - Step 311431 - Epsilon 0.9250960030786978 - Mean Reward 648.62 - Mean Length 202.95 - Mean Loss 1.058 - Mean Q Value 28.954 - Time Delta 355.033 - Time 2021-05-01T00:57:10\n",
      "Episode 1520 - Step 316304 - Epsilon 0.923969690936182 - Mean Reward 672.97 - Mean Length 209.88 - Mean Loss 1.072 - Mean Q Value 28.922 - Time Delta 471.497 - Time 2021-05-01T01:05:02\n",
      "Episode 1540 - Step 319781 - Epsilon 0.9231668791544957 - Mean Reward 654.97 - Mean Length 202.2 - Mean Loss 1.078 - Mean Q Value 29.024 - Time Delta 328.921 - Time 2021-05-01T01:10:31\n",
      "Episode 1560 - Step 324583 - Epsilon 0.9220592821455924 - Mean Reward 669.71 - Mean Length 206.34 - Mean Loss 1.088 - Mean Q Value 29.083 - Time Delta 475.498 - Time 2021-05-01T01:18:26\n",
      "Episode 1580 - Step 330639 - Epsilon 0.9206643404551808 - Mean Reward 700.45 - Mean Length 229.45 - Mean Loss 1.091 - Mean Q Value 29.054 - Time Delta 619.982 - Time 2021-05-01T01:28:46\n",
      "Episode 1600 - Step 334033 - Epsilon 0.9198834879880953 - Mean Reward 702.87 - Mean Length 226.02 - Mean Loss 1.097 - Mean Q Value 29.052 - Time Delta 323.03 - Time 2021-05-01T01:34:09\n",
      "Episode 1620 - Step 337637 - Epsilon 0.9190550461308478 - Mean Reward 692.37 - Mean Length 213.33 - Mean Loss 1.075 - Mean Q Value 29.182 - Time Delta 340.192 - Time 2021-05-01T01:39:49\n",
      "Episode 1640 - Step 341292 - Epsilon 0.9182156430380904 - Mean Reward 694.55 - Mean Length 215.11 - Mean Loss 1.094 - Mean Q Value 29.172 - Time Delta 381.26 - Time 2021-05-01T01:46:11\n",
      "Episode 1660 - Step 344167 - Epsilon 0.917555912581134 - Mean Reward 664.11 - Mean Length 195.84 - Mean Loss 1.09 - Mean Q Value 29.234 - Time Delta 284.166 - Time 2021-05-01T01:50:55\n",
      "Episode 1680 - Step 347753 - Epsilon 0.9167336922179579 - Mean Reward 626.87 - Mean Length 171.14 - Mean Loss 1.097 - Mean Q Value 29.365 - Time Delta 339.92 - Time 2021-05-01T01:56:35\n",
      "Episode 1700 - Step 351468 - Epsilon 0.9158826709489855 - Mean Reward 631.13 - Mean Length 174.35 - Mean Loss 1.099 - Mean Q Value 29.554 - Time Delta 351.256 - Time 2021-05-01T02:02:26\n",
      "Episode 1720 - Step 356708 - Epsilon 0.9146836500301699 - Mean Reward 630.83 - Mean Length 190.71 - Mean Loss 1.107 - Mean Q Value 29.714 - Time Delta 489.905 - Time 2021-05-01T02:10:36\n",
      "Episode 1740 - Step 360858 - Epsilon 0.9137351577400274 - Mean Reward 647.26 - Mean Length 195.66 - Mean Loss 1.09 - Mean Q Value 29.88 - Time Delta 405.388 - Time 2021-05-01T02:17:21\n",
      "Episode 1760 - Step 363704 - Epsilon 0.9130852663202567 - Mean Reward 641.75 - Mean Length 195.37 - Mean Loss 1.095 - Mean Q Value 29.936 - Time Delta 272.121 - Time 2021-05-01T02:21:53\n",
      "Episode 1780 - Step 369191 - Epsilon 0.9118336001323697 - Mean Reward 668.15 - Mean Length 214.38 - Mean Loss 1.082 - Mean Q Value 29.968 - Time Delta 534.134 - Time 2021-05-01T02:30:47\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1800 - Step 374294 - Epsilon 0.910671069978118 - Mean Reward 674.94 - Mean Length 228.26 - Mean Loss 1.092 - Mean Q Value 29.836 - Time Delta 497.663 - Time 2021-05-01T02:39:05\n",
      "Episode 1820 - Step 377163 - Epsilon 0.9100181252617593 - Mean Reward 661.72 - Mean Length 204.55 - Mean Loss 1.104 - Mean Q Value 29.555 - Time Delta 270.541 - Time 2021-05-01T02:43:36\n",
      "Episode 1840 - Step 380465 - Epsilon 0.9092672151861654 - Mean Reward 639.48 - Mean Length 196.07 - Mean Loss 1.114 - Mean Q Value 29.569 - Time Delta 315.644 - Time 2021-05-01T02:48:51\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "In this tutorial, we saw how we can use PyTorch to train a game-playing AI. You can use the same methods\n",
    "to train an AI to play any of the games at the `OpenAI gym <https://gym.openai.com/>`__. Hope you enjoyed this tutorial, feel free to reach us at\n",
    "`our github <https://github.com/yuansongFeng/MadMario/>`__!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
